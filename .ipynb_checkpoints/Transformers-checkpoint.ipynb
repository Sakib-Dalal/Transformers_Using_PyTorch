{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74314b36-0b7e-4039-bb0b-25d66548ed31",
   "metadata": {},
   "source": [
    "# Understanding Transformers using PyTorch\n",
    "https://www.geeksforgeeks.org/deep-learning/transformer-using-pytorch/\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250325174552667398/transformer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f15a6c2-6359-4e81-9fbe-db2b3ca7a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5016-9dcb-44ab-b85f-67767264add2",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "dea987b9-ad60-4809-97bc-ef32b5abfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model # embedding size (e.g. 512)\n",
    "        self.num_heads = num_heads # number of attention heads (e.g. 8)\n",
    "\n",
    "        # Ensuring each head gets equal dimensions\n",
    "        self.d_k = d_model // num_heads # (e.g. 512 // 8 = 64)\n",
    "        print(f\"Number of dimensions each head gets: {self.d_k}\")\n",
    "\n",
    "        # Creating Q(Query), K(Key), V(Value) from the input embeddings.\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output Layer: This combines all attention heads back into one vector.\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            This is the core attention computation.\n",
    "            Compute attention scores by taking the dot product of Q and K, scaling the result and applying softmax to normalise.\n",
    "            - Measures similarity between Q and K\n",
    "            - Division by √d_k prevents extremely large values → stabilizes training\n",
    "            - Mask used for:\n",
    "                - Padding mask\n",
    "                - Causal (future-token) masking\n",
    "            - Apply softmax = Converts scores into probabilities.\n",
    "            - Softmax example: \n",
    "                tensor([-0.8058, -0.9375,  1.2299,  0.2358, -1.0952,  0.0997,  0.8335,  2.3506, -0.3834,  0.1132]) ----> \n",
    "                tensor([0.0207, 0.0182, 0.1587, 0.0587, 0.0155, 0.0512, 0.1067, 0.4867, 0.0316, 0.0519])\n",
    "            - Multiply attn_probs with V\n",
    "        '''\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # (batch, heads, seq_len, seq_len) seq_len = -1 and -2\n",
    "        print(attn_scores.shape)\n",
    "\n",
    "        # Applying mask if needed,\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Converts scores into probabilities.\n",
    "        attn_probs = torch.softmax(attn_probs, dim=-1)\n",
    "\n",
    "        # Multiply attn_probs with V\n",
    "        output = torch.matmul(attn_probs, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        '''\n",
    "            - Input Shape: (batch_size, seq_length, d_model)\n",
    "            - Transform to: (batch_size, num_heads, seq_length, d_k)\n",
    "            - ✔ Allows parallel attention across heads.\n",
    "        '''\n",
    "        batch_size, seq_length, d_model = x.size() # example: (32, 512, 512)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # example: (32, 8, 512, 64)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        '''\n",
    "            - Input shape: (batch, heads, seq_len, d_k)\n",
    "            - Output shape: (batch, seq_len, d_model)\n",
    "            - ✔ Merges all heads into a single vector.\n",
    "        '''\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)   # Returns a contiguous in-memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553e83a-3be2-4392-bd37-badb4c8bad5d",
   "metadata": {},
   "source": [
    "### Shape Summary\n",
    "\n",
    "| Step          | Shape                          |\n",
    "| ------------- | ------------------------------ |\n",
    "| Input         | `(batch, seq_len, d_model)`    |\n",
    "| Split heads   | `(batch, heads, seq_len, d_k)` |\n",
    "| Attention     | `(batch, heads, seq_len, d_k)` |\n",
    "| Combine heads | `(batch, seq_len, d_model)`    |\n",
    "| Final output  | `(batch, seq_len, d_model)`    |\n",
    "\n",
    "### Why Multi-Head Attention is Powerful\n",
    "\n",
    "- ✔ Captures multiple relationships\n",
    "- ✔ Works in parallel (faster than RNNs)\n",
    "- ✔ Handles long-range dependencies\n",
    "- ✔ Foundation of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a871054-8118-4814-b1fe-b64ab8f0a511",
   "metadata": {},
   "source": [
    "## Position-Wise Feed Forward\n",
    "\n",
    "In a Transformer, after attention mixes information across tokens, the FFN:\n",
    "- Processes each token independently\n",
    "- Adds non-linearity\n",
    "- Expands and compresses feature space\n",
    "\n",
    "Think of it as a small neural network applied to every token separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a33eebf5-ff89-489c-a5ce-1d32939ea328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        '''\n",
    "            d_model: embedding size\n",
    "            d_ff: hidden size of FFN\n",
    "        '''\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = nn.ReLU() # introduces non-linearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8cd2-f867-431f-b0a3-a52b8c16f91b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "This block defines the Positional Encoding class, which adds positional information to the token embeddings, allowing the model to retain information about word positions in the input sequence.\n",
    "\n",
    "This class implements Sinusoidal Positional Encoding, which is essential for Transformers because attention alone has no sense of order.\n",
    "\n",
    "Transformers:\n",
    "- Do not use RNNs or CNNs\n",
    "- Process all tokens in parallel\n",
    "- Have no inherent notion of sequence order\n",
    "\n",
    "So we inject position information into embeddings.\n",
    "\n",
    "This class creates fixed (non-learned) sinusoidal positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca58ca0b-0a9c-438e-bb84-16737d3257a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create Empty Positional Encoding Matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        '''\n",
    "            creates:\n",
    "                [[0],\n",
    "                 [1],\n",
    "                 [2],\n",
    "                 ...\n",
    "                 [max_seq_length-1]]\n",
    "            shape:\n",
    "                (max_seq_length, 1)\n",
    "        '''\n",
    "\n",
    "        # This generates different wavelengths for different dimensions.\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply Sine to Even Indices # Encodes position smoothly\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply Cosine to Odd Indices # Paired with sine for phase-shift encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # register_buffer so that during training it doesn't get updated \n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea029d0-edb0-4039-b97c-cad5191bfd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (CloudLab)",
   "language": "python",
   "name": "cloudlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
