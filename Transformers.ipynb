{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74314b36-0b7e-4039-bb0b-25d66548ed31",
   "metadata": {},
   "source": [
    "# Understanding Transformers using PyTorch\n",
    "https://www.geeksforgeeks.org/deep-learning/transformer-using-pytorch/\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250325174552667398/transformer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f15a6c2-6359-4e81-9fbe-db2b3ca7a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5016-9dcb-44ab-b85f-67767264add2",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "dea987b9-ad60-4809-97bc-ef32b5abfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model # embedding size (e.g. 512)\n",
    "        self.num_heads = num_heads # number of attention heads (e.g. 8)\n",
    "\n",
    "        # Ensuring each head gets equal dimensions\n",
    "        self.d_k = d_model // num_heads # (e.g. 512 // 8 = 64)\n",
    "        print(f\"Number of dimensions each head gets: {self.d_k}\")\n",
    "\n",
    "        # Creating Q(Query), K(Key), V(Value) from the input embeddings.\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output Layer: This combines all attention heads back into one vector.\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            This is the core attention computation.\n",
    "            Compute attention scores by taking the dot product of Q and K, scaling the result and applying softmax to normalise.\n",
    "            - Measures similarity between Q and K\n",
    "            - Division by √d_k prevents extremely large values → stabilizes training\n",
    "            - Mask used for:\n",
    "                - Padding mask\n",
    "                - Causal (future-token) masking\n",
    "            - Apply softmax = Converts scores into probabilities.\n",
    "            - Softmax example: \n",
    "                tensor([-0.8058, -0.9375,  1.2299,  0.2358, -1.0952,  0.0997,  0.8335,  2.3506, -0.3834,  0.1132]) ----> \n",
    "                tensor([0.0207, 0.0182, 0.1587, 0.0587, 0.0155, 0.0512, 0.1067, 0.4867, 0.0316, 0.0519])\n",
    "            - Multiply attn_probs with V\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # (batch, heads, seq_len, seq_len) seq_len = -1 and -2\n",
    "\n",
    "        # Applying mask if needed,\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Converts scores into probabilities.\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Multiply attn_probs with V\n",
    "        output = torch.matmul(attn, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        '''\n",
    "            - Input Shape: (batch_size, seq_length, d_model)\n",
    "            - Transform to: (batch_size, num_heads, seq_length, d_k)\n",
    "            - ✔ Allows parallel attention across heads.\n",
    "        '''\n",
    "        batch_size, seq_length, d_model = x.size() # example: (32, 512, 512)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # example: (32, 8, 512, 64)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        '''\n",
    "            - Input shape: (batch, heads, seq_len, d_k)\n",
    "            - Output shape: (batch, seq_len, d_model)\n",
    "            - ✔ Merges all heads into a single vector.\n",
    "        '''\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)   # Returns a contiguous in-memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553e83a-3be2-4392-bd37-badb4c8bad5d",
   "metadata": {},
   "source": [
    "### Shape Summary\n",
    "\n",
    "| Step          | Shape                          |\n",
    "| ------------- | ------------------------------ |\n",
    "| Input         | `(batch, seq_len, d_model)`    |\n",
    "| Split heads   | `(batch, heads, seq_len, d_k)` |\n",
    "| Attention     | `(batch, heads, seq_len, d_k)` |\n",
    "| Combine heads | `(batch, seq_len, d_model)`    |\n",
    "| Final output  | `(batch, seq_len, d_model)`    |\n",
    "\n",
    "### Why Multi-Head Attention is Powerful\n",
    "\n",
    "- ✔ Captures multiple relationships\n",
    "- ✔ Works in parallel (faster than RNNs)\n",
    "- ✔ Handles long-range dependencies\n",
    "- ✔ Foundation of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a871054-8118-4814-b1fe-b64ab8f0a511",
   "metadata": {},
   "source": [
    "## Position-Wise Feed Forward\n",
    "\n",
    "In a Transformer, after attention mixes information across tokens, the FFN:\n",
    "- Processes each token independently\n",
    "- Adds non-linearity\n",
    "- Expands and compresses feature space\n",
    "\n",
    "Think of it as a small neural network applied to every token separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a33eebf5-ff89-489c-a5ce-1d32939ea328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        '''\n",
    "            d_model: embedding size\n",
    "            d_ff: hidden size of FFN\n",
    "        '''\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = nn.ReLU() # introduces non-linearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8cd2-f867-431f-b0a3-a52b8c16f91b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "This block defines the Positional Encoding class, which adds positional information to the token embeddings, allowing the model to retain information about word positions in the input sequence.\n",
    "\n",
    "This class implements Sinusoidal Positional Encoding, which is essential for Transformers because attention alone has no sense of order.\n",
    "\n",
    "Transformers:\n",
    "- Do not use RNNs or CNNs\n",
    "- Process all tokens in parallel\n",
    "- Have no inherent notion of sequence order\n",
    "\n",
    "So we inject position information into embeddings.\n",
    "\n",
    "This class creates fixed (non-learned) sinusoidal positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "ca58ca0b-0a9c-438e-bb84-16737d3257a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create Empty Positional Encoding Matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        '''\n",
    "            creates:\n",
    "                [[0],\n",
    "                 [1],\n",
    "                 [2],\n",
    "                 ...\n",
    "                 [max_seq_length-1]]\n",
    "            shape:\n",
    "                (max_seq_length, 1)\n",
    "        '''\n",
    "\n",
    "        # This generates different wavelengths for different dimensions.\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply Sine to Even Indices # Encodes position smoothly\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply Cosine to Odd Indices # Paired with sine for phase-shift encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # register_buffer so that during training it doesn't get updated \n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e69af6-8b24-49e3-9ea3-f699845f20b7",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "This block defines the Encoder Layer class, which contains the multi-head attention mechanism and the position-wise feed-forward network, with layer normalisation and dropout applied.\n",
    "\n",
    "This class is one complete Transformer Encoder block.\n",
    "\n",
    "It combines everything you’ve learned so far:\n",
    "- Multi-Head Self-Attention\n",
    "- Position-wise Feed-Forward Network\n",
    "- Residual connections\n",
    "- Layer Normalisation\n",
    "- Dropout\n",
    "\n",
    "#### What Is an Encoder Layer?\n",
    "In a Transformer encoder, each layer does two things:\n",
    "- Self-attention → tokens look at each other\n",
    "- Feed-forward network → each token is refined individually\n",
    "\n",
    "This block is stacked N times (e.g., 6 or 12 layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "cc401dee-aa5d-4530-aeee-ec7d56eb673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        '''\n",
    "            - d_model: embedding size (e.g. 512)\n",
    "            - num_heads: attention heads (e.g. 8)\n",
    "            - d_ff: FFN hidden size (e.g. 2048)\n",
    "            - dropout: regularisation probability\n",
    "        '''\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "099d533a-5162-4db4-ac2d-d6339f719e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions each head gets: 64\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderLayer(512, 8, 2048, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "02fc122d-a0e3-4461-b3c3-b11f9fe1fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (self_attn): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (feed_forward): PositionWiseFeedForward(\n",
       "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "9e020e07-e960-44ca-861f-05d414ed36b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0620, -0.5188, -1.4813, -0.3911, -0.1161,  0.0293, -2.4177,\n",
       "           0.8640],\n",
       "         [-1.0250, -1.9186,  0.5580, -1.3278,  0.0911,  2.6778, -1.0107,\n",
       "           0.5521],\n",
       "         [-0.5026, -1.4435, -1.2836,  0.0404, -1.4624, -0.6389,  2.6402,\n",
       "          -0.2110],\n",
       "         [-0.1491,  0.2950,  0.9525,  0.4953,  0.0340, -1.3236,  0.1164,\n",
       "          -0.1951],\n",
       "         [-1.3562, -1.1630,  0.8303,  1.5787, -0.6076, -1.3429,  0.4135,\n",
       "          -0.5871]],\n",
       "\n",
       "        [[ 0.3376,  0.4662,  0.3402, -1.0618,  0.5530,  0.8334, -0.1288,\n",
       "           0.8086],\n",
       "         [-1.2023, -0.9707,  0.9154,  1.1936,  1.7496,  0.6047, -0.5812,\n",
       "           0.6910],\n",
       "         [-1.2275, -0.3438,  0.6108, -0.8516,  0.8066, -0.2044, -1.7381,\n",
       "           0.5873],\n",
       "         [-0.0929,  0.4559,  1.3546,  0.7567, -0.1896,  1.8262, -1.1430,\n",
       "          -0.6867],\n",
       "         [-1.9211,  0.0942, -0.0342, -0.8717, -0.0406,  1.1138,  0.4076,\n",
       "           0.1987]]])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_ff = 32\n",
    "dropout = 0.1\n",
    "\n",
    "# Random input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "819c95a2-9fb2-4ade-a438-6486fe655cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions each head gets: 4\n",
      "Input shape : torch.Size([2, 5, 8])\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "\n",
      "Sample output tensor:\n",
      " tensor([[[ 0.3806,  0.1950, -0.2536, -1.4823, -1.0373, -0.2126,  2.0797,\n",
      "           0.3305],\n",
      "         [ 0.2097, -1.2543, -0.2978,  0.7728, -1.4621, -0.4739,  0.9399,\n",
      "           1.5656],\n",
      "         [-0.5283,  0.2674, -0.0336, -0.6224, -1.2941,  0.0087,  2.3583,\n",
      "          -0.1560],\n",
      "         [ 0.9262,  1.6344, -1.0152, -0.0511, -0.5642, -1.3574, -0.5562,\n",
      "           0.9835],\n",
      "         [ 1.1277, -0.0372, -0.9250, -0.9798, -1.4394,  0.4068,  1.6193,\n",
      "           0.2276]],\n",
      "\n",
      "        [[-0.3407, -1.3028, -0.3811,  2.1654,  0.0465, -0.7679,  0.8343,\n",
      "          -0.2536],\n",
      "         [-0.1522, -1.0708,  0.3779,  0.4899,  1.6798,  0.0306, -1.8409,\n",
      "           0.4857],\n",
      "         [ 0.2907,  1.6394, -0.8440,  0.6338, -0.5269, -0.1237, -1.8092,\n",
      "           0.7399],\n",
      "         [-0.3635,  1.2514,  0.0345,  1.3420,  0.8607, -0.9406, -1.5861,\n",
      "          -0.5984],\n",
      "         [ 0.6888, -0.7602, -0.7840,  1.3678, -0.6249,  1.6699, -0.5913,\n",
      "          -0.9661]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Optional mask (1 = keep, 0 = mask)\n",
    "mask = torch.ones(batch_size, 1, 1, seq_len)\n",
    "\n",
    "# Encoder\n",
    "encoder = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder(x, mask)\n",
    "\n",
    "# Results\n",
    "print(\"Input shape :\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"\\nSample output tensor:\\n\", output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42036e1e-9e08-4419-b125-0b920af5e3b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (CloudLab)",
   "language": "python",
   "name": "cloudlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
