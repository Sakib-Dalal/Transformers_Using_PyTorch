{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74314b36-0b7e-4039-bb0b-25d66548ed31",
   "metadata": {},
   "source": [
    "# Understanding Transformers using PyTorch\n",
    "https://www.geeksforgeeks.org/deep-learning/transformer-using-pytorch/\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20250325174552667398/transformer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f15a6c2-6359-4e81-9fbe-db2b3ca7a600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ac5016-9dcb-44ab-b85f-67767264add2",
   "metadata": {},
   "source": [
    "## MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dea987b9-ad60-4809-97bc-ef32b5abfaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model # embedding size (e.g. 512)\n",
    "        self.num_heads = num_heads # number of attention heads (e.g. 8)\n",
    "\n",
    "        # Ensuring each head gets equal dimensions\n",
    "        self.d_k = d_model // num_heads # (e.g. 512 // 8 = 64)\n",
    "        print(f\"Number of dimensions each head gets: {self.d_k}\")\n",
    "\n",
    "        # Creating Q(Query), K(Key), V(Value) from the input embeddings.\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # Output Layer: This combines all attention heads back into one vector.\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        '''\n",
    "            This is the core attention computation.\n",
    "            Compute attention scores by taking the dot product of Q and K, scaling the result and applying softmax to normalise.\n",
    "            - Measures similarity between Q and K\n",
    "            - Division by √d_k prevents extremely large values → stabilizes training\n",
    "            - Mask used for:\n",
    "                - Padding mask\n",
    "                - Causal (future-token) masking\n",
    "            - Apply softmax = Converts scores into probabilities.\n",
    "            - Softmax example: \n",
    "                tensor([-0.8058, -0.9375,  1.2299,  0.2358, -1.0952,  0.0997,  0.8335,  2.3506, -0.3834,  0.1132]) ----> \n",
    "                tensor([0.0207, 0.0182, 0.1587, 0.0587, 0.0155, 0.0512, 0.1067, 0.4867, 0.0316, 0.0519])\n",
    "            - Multiply attn_probs with V\n",
    "        '''\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # (batch, heads, seq_len, seq_len) seq_len = -1 and -2\n",
    "\n",
    "        # Applying mask if needed,\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # Converts scores into probabilities.\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        # Multiply attn_probs with V\n",
    "        output = torch.matmul(attn, V)\n",
    "        return output\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        '''\n",
    "            - Input Shape: (batch_size, seq_length, d_model)\n",
    "            - Transform to: (batch_size, num_heads, seq_length, d_k)\n",
    "            - ✔ Allows parallel attention across heads.\n",
    "        '''\n",
    "        batch_size, seq_length, d_model = x.size() # example: (32, 512, 512)\n",
    "        return x.view(batch_size, seq_length, self.num_heads, self.d_k).transpose(1, 2) # example: (32, 8, 512, 64)\n",
    "        \n",
    "    def combine_heads(self, x):\n",
    "        '''\n",
    "            - Input shape: (batch, heads, seq_len, d_k)\n",
    "            - Output shape: (batch, seq_len, d_model)\n",
    "            - ✔ Merges all heads into a single vector.\n",
    "        '''\n",
    "        batch_size, _, seq_len, d_k = x.size()\n",
    "        return x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)   # Returns a contiguous in-memory tensor containing the same data as self tensor. If self tensor is already in the specified memory format, this function returns the self tensor.\n",
    "\n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "\n",
    "        output = self.W_o(self.combine_heads(attn_output))\n",
    "        return output        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b553e83a-3be2-4392-bd37-badb4c8bad5d",
   "metadata": {},
   "source": [
    "### Shape Summary\n",
    "\n",
    "| Step          | Shape                          |\n",
    "| ------------- | ------------------------------ |\n",
    "| Input         | `(batch, seq_len, d_model)`    |\n",
    "| Split heads   | `(batch, heads, seq_len, d_k)` |\n",
    "| Attention     | `(batch, heads, seq_len, d_k)` |\n",
    "| Combine heads | `(batch, seq_len, d_model)`    |\n",
    "| Final output  | `(batch, seq_len, d_model)`    |\n",
    "\n",
    "### Why Multi-Head Attention is Powerful\n",
    "\n",
    "- ✔ Captures multiple relationships\n",
    "- ✔ Works in parallel (faster than RNNs)\n",
    "- ✔ Handles long-range dependencies\n",
    "- ✔ Foundation of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a871054-8118-4814-b1fe-b64ab8f0a511",
   "metadata": {},
   "source": [
    "## Position-Wise Feed Forward\n",
    "\n",
    "In a Transformer, after attention mixes information across tokens, the FFN:\n",
    "- Processes each token independently\n",
    "- Adds non-linearity\n",
    "- Expands and compresses feature space\n",
    "\n",
    "Think of it as a small neural network applied to every token separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a33eebf5-ff89-489c-a5ce-1d32939ea328",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        '''\n",
    "            d_model: embedding size\n",
    "            d_ff: hidden size of FFN\n",
    "        '''\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.relu = nn.ReLU() # introduces non-linearity\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765b8cd2-f867-431f-b0a3-a52b8c16f91b",
   "metadata": {},
   "source": [
    "### Positional Encoding\n",
    "This block defines the Positional Encoding class, which adds positional information to the token embeddings, allowing the model to retain information about word positions in the input sequence.\n",
    "\n",
    "This class implements Sinusoidal Positional Encoding, which is essential for Transformers because attention alone has no sense of order.\n",
    "\n",
    "Transformers:\n",
    "- Do not use RNNs or CNNs\n",
    "- Process all tokens in parallel\n",
    "- Have no inherent notion of sequence order\n",
    "\n",
    "So we inject position information into embeddings.\n",
    "\n",
    "This class creates fixed (non-learned) sinusoidal positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca58ca0b-0a9c-438e-bb84-16737d3257a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "\n",
    "        # Create Empty Positional Encoding Matrix\n",
    "        pe = torch.zeros(max_seq_length, d_model)\n",
    "\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        '''\n",
    "            creates:\n",
    "                [[0],\n",
    "                 [1],\n",
    "                 [2],\n",
    "                 ...\n",
    "                 [max_seq_length-1]]\n",
    "            shape:\n",
    "                (max_seq_length, 1)\n",
    "        '''\n",
    "\n",
    "        # This generates different wavelengths for different dimensions.\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model)\n",
    "        )\n",
    "\n",
    "        # Apply Sine to Even Indices # Encodes position smoothly\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply Cosine to Odd Indices # Paired with sine for phase-shift encoding\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # register_buffer so that during training it doesn't get updated \n",
    "        self.register_buffer('pe', pe.unsqueeze(0)) # (1, max_seq_length, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.pe[:, :x.size(1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e69af6-8b24-49e3-9ea3-f699845f20b7",
   "metadata": {},
   "source": [
    "## Encoder Layer\n",
    "\n",
    "<img src=\"https://media.datacamp.com/legacy/v1691083306/Figure_2_The_Encoder_part_of_the_transformer_network_Source_image_from_the_original_paper_b0e3ac40fa.png\">\n",
    "\n",
    "This block defines the Encoder Layer class, which contains the multi-head attention mechanism and the position-wise feed-forward network, with layer normalisation and dropout applied.\n",
    "\n",
    "This class is one complete Transformer Encoder block.\n",
    "\n",
    "It combines everything you’ve learned so far:\n",
    "- Multi-Head Self-Attention\n",
    "- Position-wise Feed-Forward Network\n",
    "- Residual connections\n",
    "- Layer Normalisation\n",
    "- Dropout\n",
    "\n",
    "#### What Is an Encoder Layer?\n",
    "In a Transformer encoder, each layer does two things:\n",
    "- Self-attention → tokens look at each other\n",
    "- Feed-forward network → each token is refined individually\n",
    "\n",
    "This block is stacked N times (e.g., 6 or 12 layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc401dee-aa5d-4530-aeee-ec7d56eb673d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        '''\n",
    "            - d_model: embedding size (e.g. 512)\n",
    "            - num_heads: attention heads (e.g. 8)\n",
    "            - d_ff: FFN hidden size (e.g. 2048)\n",
    "            - dropout: regularisation probability\n",
    "        '''\n",
    "        \n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        attn_output = self.self_attn(x, x, x, mask)\n",
    "        x = self.norm1(x + self.dropout(attn_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "099d533a-5162-4db4-ac2d-d6339f719e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions each head gets: 64\n"
     ]
    }
   ],
   "source": [
    "encoder = EncoderLayer(512, 8, 2048, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "02fc122d-a0e3-4461-b3c3-b11f9fe1fe5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderLayer(\n",
       "  (self_attn): MultiHeadAttention(\n",
       "    (W_q): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_k): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_v): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (W_o): Linear(in_features=512, out_features=512, bias=True)\n",
       "  )\n",
       "  (feed_forward): PositionWiseFeedForward(\n",
       "    (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
       "    (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (relu): ReLU()\n",
       "  )\n",
       "  (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e020e07-e960-44ca-861f-05d414ed36b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.2160, -1.4414,  0.9248,  0.8221, -0.7134, -0.5704,  0.9568,\n",
       "           0.5216],\n",
       "         [ 0.6904,  0.5042, -0.3047, -1.8446, -0.9328, -0.6943, -0.1510,\n",
       "          -1.1770],\n",
       "         [-0.6040,  2.2395, -0.7380,  1.1772,  0.5163,  1.3221,  1.1290,\n",
       "           0.4016],\n",
       "         [-1.3940, -0.1856,  1.7577, -1.0043, -0.1822,  0.2720,  0.2024,\n",
       "           2.0691],\n",
       "         [-1.1455, -0.2866,  0.0954, -0.3493, -1.8010,  0.1306, -1.1826,\n",
       "           0.1104]],\n",
       "\n",
       "        [[ 1.0447,  1.0268,  0.0237, -1.0587,  2.6600, -1.0581, -1.0108,\n",
       "           2.4103],\n",
       "         [-0.2807,  0.1621,  0.3188, -0.3377,  1.0698, -0.2189,  0.0544,\n",
       "           0.4956],\n",
       "         [ 1.2000,  0.4260,  0.0424,  0.9311, -0.1950, -0.9808, -0.2860,\n",
       "          -0.5460],\n",
       "         [ 0.1338,  0.7567,  1.0144,  0.3682, -0.3540,  0.5332, -0.1951,\n",
       "           0.9310],\n",
       "         [ 0.6857, -0.1839,  2.0093, -0.5572,  0.5208,  1.3920,  0.7627,\n",
       "          -0.0888]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "batch_size = 2\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_ff = 32\n",
    "dropout = 0.1\n",
    "\n",
    "# Random input\n",
    "x = torch.randn(batch_size, seq_len, d_model)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "819c95a2-9fb2-4ade-a438-6486fe655cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dimensions each head gets: 4\n",
      "Input shape : torch.Size([2, 5, 8])\n",
      "Output shape: torch.Size([2, 5, 8])\n",
      "\n",
      "Sample output tensor:\n",
      " tensor([[[ 0.1160, -1.9058,  1.2555,  0.4634, -0.0704, -1.2425,  0.8607,\n",
      "           0.5231],\n",
      "         [ 1.4181,  1.1662,  0.5641, -1.6535,  0.3372, -0.7753, -0.1390,\n",
      "          -0.9179],\n",
      "         [-0.6727,  1.8312, -1.7930,  0.4063, -0.4296,  0.6819,  0.2747,\n",
      "          -0.2988],\n",
      "         [-1.2074, -0.4310,  1.5404, -1.2539,  0.5126, -0.3134, -0.2561,\n",
      "           1.4088],\n",
      "         [-0.5458,  0.4601,  1.6484, -0.3289, -1.2459,  0.3304, -1.3742,\n",
      "           1.0559]],\n",
      "\n",
      "        [[ 0.3426,  0.1328, -0.3007, -0.6316,  2.0421, -1.2133, -1.0981,\n",
      "           0.7263],\n",
      "         [-0.5920, -0.0828,  0.1886, -0.5443,  2.3220, -1.1768, -0.5710,\n",
      "           0.4563],\n",
      "         [ 1.6055,  0.5703, -0.2807,  1.1716, -0.0110, -1.6115, -0.6693,\n",
      "          -0.7750],\n",
      "         [-0.4545,  1.0567,  1.6231, -0.4368, -0.9167, -0.6329, -1.2557,\n",
      "           1.0167],\n",
      "         [ 0.5090, -0.9316,  1.8818, -1.5946,  0.7110,  0.1406, -0.2978,\n",
      "          -0.4185]]], grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Optional mask (1 = keep, 0 = mask)\n",
    "mask = torch.ones(batch_size, 1, 1, seq_len)\n",
    "\n",
    "# Encoder\n",
    "encoder = EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "\n",
    "# Forward pass\n",
    "output = encoder(x, mask)\n",
    "\n",
    "# Results\n",
    "print(\"Input shape :\", x.shape)\n",
    "print(\"Output shape:\", output.shape)\n",
    "print(\"\\nSample output tensor:\\n\", output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42036e1e-9e08-4419-b125-0b920af5e3b5",
   "metadata": {},
   "source": [
    "## Decoder Layer \n",
    "\n",
    "<img src=\"https://media.datacamp.com/legacy/v1691083444/Figure_3_The_Decoder_part_of_the_Transformer_network_Souce_Image_from_the_original_paper_b90d9e7f66.png\">\n",
    "\n",
    "This block defines the Decoder Layer class, which is similar to the encoder layer but also includes a cross-attention mechanism to attend to the encoder’s output.\n",
    "\n",
    "It is more complex than the encoder because the decoder must:\n",
    "- Look at past generated tokens only (masked self-attention)\n",
    "- Look at the encoder output (cross-attention)\n",
    "- Refine features with a feed-forward network\n",
    "\n",
    "The decoder block has 3 sub-blocks\n",
    "1) Masked Self-Attention\n",
    "2) Encoder–Decoder (Cross) Attention\n",
    "3) Feed-Forward Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c9828de0-7cf0-4d8a-8b15-b008a4dd8d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        # Masked Self-Attention\n",
    "        self.self_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "        \n",
    "        # Encoder-Decoder (Cross) Attention\n",
    "        self.cross_attn = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "\n",
    "        # Feed-Forward Network\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model=d_model, d_ff=d_ff)\n",
    "\n",
    "        # Normalization\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask, tgt_mask):\n",
    "        '''\n",
    "            x: decoder input\n",
    "            (batch, tgt_seq_len, d_model)\n",
    "            \n",
    "            enc_output: encoder output\n",
    "            (batch, src_seq_len, d_model)\n",
    "            \n",
    "            src_mask: mask for encoder tokens (padding)\n",
    "            \n",
    "            tgt_mask: mask for decoder tokens (causal + padding)\n",
    "        '''\n",
    "\n",
    "        # Masked Self-Attention (Decoder → Decoder)\n",
    "        # Q = K = V = x | tgt_mask prevents attending to future tokens | Enables autoregressive generation\n",
    "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
    "\n",
    "        x = self.norm1(x + self.dropout(attn_output)) # Stabilizes gradients and preserves information.\n",
    "\n",
    "        # Cross-Attention (Decoder → Encoder)\n",
    "        # Query = decoder states (x) | Key & Value = encoder output (enc_output) | src_mask hides encoder padding tokens\n",
    "        attn_output = self.cross_attn(x, enc_output, enc_output, src_mask)\n",
    "\n",
    "        x = self.norm2(x + self.dropout(attn_output))\n",
    "\n",
    "        # Feed Forward Network\n",
    "        ff_output = self.feed_forward(x)\n",
    "\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "\n",
    "        return x  # Output shape: (batch, tgt_seq_len, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ad153f-47cd-4943-ad57-3812cb8b1843",
   "metadata": {},
   "source": [
    "## Transformer Model\n",
    "\n",
    "<img src=\"https://media.datacamp.com/legacy/v1691083566/Figure_4_The_Transformer_Network_Source_Image_from_the_original_paper_120e177956.png\">\n",
    "\n",
    "This block defines the main Transformer class which combines the encoder and decoder layers. It also includes the embedding layers and the final output layer.\n",
    "\n",
    "This model is designed for sequence-to-sequence tasks, such as:\n",
    "- Machine Translation\n",
    "- Text Summarization\n",
    "- Question Answering\n",
    "- Code Generation\n",
    "\n",
    "```\n",
    "Source sentence (src)\n",
    "   ↓\n",
    "Encoder (understands input)\n",
    "   ↓\n",
    "Decoder (generates output)\n",
    "   ↓\n",
    "Vocabulary logits\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "752da0ff-1382-4309-ad85-fd7baef29df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
    "        '''\n",
    "            - src_vocab_size: vocabulary size of source language\n",
    "            - tgt_vocab_size: vocabulary size of target language\n",
    "            - d_model: embedding dimension\n",
    "            - num_heads: attention heads\n",
    "            - num_layers: encoder/decoder layers\n",
    "            - d_ff: feed-forward hidden dimension\n",
    "            - max_seq_length: maximum sequence length\n",
    "            - dropout: regularization rate\n",
    "        '''\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # Convert token IDs → dense vectors\n",
    "        self.endoder_embedding = nn.Embedding(num_embeddings=src_vocab_size, embedding_dim=d_model)\n",
    "        self.decoder_embedding = nn.Embedding(num_embeddings=tgt_vocab_size, embedding_dim=d_model)\n",
    "\n",
    "        # Add positional information so order matters\n",
    "        self.positional_encoding = PositionalEncoding(d_model=d_model, max_seq_length=max_seq_length)\n",
    "\n",
    "        '''\n",
    "            Creates a stack of encoder layers.\n",
    "            Each encoder layer:\n",
    "                Self-attention\n",
    "                Feed-forward\n",
    "                Add & Norm\n",
    "        '''\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        ) # Holds submodules in a list.\n",
    "\n",
    "        '''\n",
    "            Creates a stack of decoder layers.\n",
    "            Each decoder layer:\n",
    "                Masked self-attention\n",
    "                Cross-attention (encoder–decoder)\n",
    "                Feed-forward\n",
    "        '''\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        ) # Holds submodules in a list.\n",
    "\n",
    "        # Converts decoder outputs into logits over target vocabulary.\n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        ''' This is crucial for correct Transformer behavior. '''\n",
    "\n",
    "        device = src.device\n",
    "        \n",
    "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2) # Shape: (batch, 1, 1, src_len) # Masks out <PAD> tokens in the encoder\n",
    "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(3) # Shape: (batch, 1, tgt_len, 1) # Masks <PAD> tokens in decoder input.\n",
    "\n",
    "        seq_length = tgt.size(1) \n",
    "\n",
    "        # No-Peek (Causal) Mask: Creates a lower-triangular matrix:\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length, device=device), diagonal=1)).bool() # Prevents decoder from seeing future tokens\n",
    "        \n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        '''\n",
    "            src: source token IDs (batch, src_len)\n",
    "            tgt: target token IDs (batch, tgt_len)\n",
    "        '''\n",
    "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
    "\n",
    "        src_embedded = self.dropout(self.positional_encoding(self.endoder_embedding(src)))\n",
    "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
    "\n",
    "        enc_output = src_embedded\n",
    "        for enc_layer in self.encoder_layers:\n",
    "            enc_output = enc_layer(enc_output, src_mask)\n",
    "\n",
    "        dec_output = tgt_embedded\n",
    "        for dec_layer in self.decoder_layers:\n",
    "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        output = self.fc(dec_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3c195a-863d-4b23-974b-6e4531ea37db",
   "metadata": {},
   "source": [
    "## Training and Testing the Transformer using the dummy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "55804c4f-a339-4f43-aa23-110d9a1af85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DummyTranslationDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000, seq_len=10, vocab_size=50):\n",
    "        self.data = []\n",
    "        for _ in range(num_samples):\n",
    "            src = torch.randint(3, vocab_size, (seq_len,))\n",
    "            tgt = torch.randint(3, vocab_size, (seq_len,))\n",
    "            self.data.append((src, tgt))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4761a6a8-4068-441e-9dcc-9155a69913f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = DummyTranslationDataset()\n",
    "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f681dfcc-3294-4032-b314-5e3bd7d2ebde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n",
      "Number of dimensions each head gets: 32\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "print(device)\n",
    "\n",
    "model = Transformer(\n",
    "    src_vocab_size=50,\n",
    "    tgt_vocab_size=50,\n",
    "    d_model=128,\n",
    "    num_heads=4,\n",
    "    num_layers=4,\n",
    "    d_ff=512,\n",
    "    max_seq_length=50,\n",
    "    dropout=0.1\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "403e143f-0268-4932-a8ea-b5d7d4e4ede7",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=3e-4,\n",
    "    betas=(0.9, 0.98),\n",
    "    eps=1e-9\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "24d61369-6882-42db-b24d-0e8750db693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 | Loss: 3.8924\n",
      "Epoch 2 | Loss: 3.7792\n",
      "Epoch 3 | Loss: 3.4974\n",
      "Epoch 4 | Loss: 3.1723\n",
      "Epoch 5 | Loss: 2.7903\n",
      "Epoch 6 | Loss: 2.2669\n",
      "Epoch 7 | Loss: 1.6176\n",
      "Epoch 8 | Loss: 1.1770\n",
      "Epoch 9 | Loss: 0.9573\n",
      "Epoch 10 | Loss: 0.8430\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt in loader:\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "\n",
    "        # Teacher forcing\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        logits = model(src, tgt_input)\n",
    "\n",
    "        # Reshape for loss\n",
    "        logits = logits.reshape(-1, logits.size(-1))\n",
    "        tgt_output = tgt_output.reshape(-1)\n",
    "\n",
    "        loss = criterion(logits, tgt_output)\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Loss: {total_loss / len(loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "432c57f5-07cb-4d3a-ae0e-ee1487ac41c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Loss: 0.4719\n",
      "Token Accuracy : 89.46%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    correct_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in dataloader:\n",
    "            src = src.to(device)\n",
    "            tgt = tgt.to(device)\n",
    "\n",
    "            # Teacher forcing\n",
    "            tgt_input = tgt[:, :-1]\n",
    "            tgt_output = tgt[:, 1:]\n",
    "\n",
    "            logits = model(src, tgt_input)\n",
    "\n",
    "            # Loss\n",
    "            logits_flat = logits.reshape(-1, logits.size(-1))\n",
    "            tgt_output_flat = tgt_output.reshape(-1)\n",
    "\n",
    "            loss = criterion(logits_flat, tgt_output_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Accuracy (ignore PAD tokens)\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            mask = tgt_output != 0  # PAD = 0\n",
    "\n",
    "            correct_tokens += (predictions == tgt_output).masked_select(mask).sum().item()\n",
    "            total_tokens += mask.sum().item()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_tokens / total_tokens if total_tokens > 0 else 0\n",
    "\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "eval_loss, eval_acc = evaluate(model, loader, criterion, device)\n",
    "\n",
    "print(f\"Evaluation Loss: {eval_loss:.4f}\")\n",
    "print(f\"Token Accuracy : {eval_acc * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89de4a5f-ca6f-4e5d-bae3-1e774c465059",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12 (CloudLab)",
   "language": "python",
   "name": "cloudlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
